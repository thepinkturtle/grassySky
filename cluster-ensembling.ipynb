{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import ttest_ind\nfrom sklearn import metrics\nfrom scipy import stats\nfrom statistics import mode\n\n#Importing libraries for model creation\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.mixture import BayesianGaussianMixture\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.ensemble import VotingClassifier \n\n#Importing pre-processing\nfrom sklearn import preprocessing\n#Decomposition\nfrom sklearn.decomposition import PCA\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-29T01:11:47.672697Z","iopub.execute_input":"2022-07-29T01:11:47.673150Z","iopub.status.idle":"2022-07-29T01:11:48.814646Z","shell.execute_reply.started":"2022-07-29T01:11:47.673060Z","shell.execute_reply":"2022-07-29T01:11:48.813258Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/tabular-playground-series-jul-2022/sample_submission.csv\n/kaggle/input/tabular-playground-series-jul-2022/data.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Problem Approach\n\nIn this notebook we're tasked with clustering an unlabeled dataset with the evaluation metric being the RandScore. For our model we compare a Gaussian Mixture with a Bayesian Gaussian mixture and end up setting on the Bayesian Gaussian for the final submission. The overall approach is as follows:\n\n* Data Loading\n* Data Normalization\n* Feature Selection\n* Determining Optimal Number of Clusters\n* Training models: Gaussian Mixture, Bayesian Gausian Mixture, Mini K means\n* Ensembling predictions\n* Submitting result from ensembled model","metadata":{}},{"cell_type":"markdown","source":"## Loading Data","metadata":{}},{"cell_type":"code","source":"# df = pd.read_csv('../input/tabular-playground-series-jul-2022/data.csv', index_col=False)\n# df = df.fillna(0)\ndf = pd.read_csv('/kaggle/input/tabular-playground-series-jul-2022/data.csv', index_col=False)\ndf = df.fillna(0)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T01:11:48.816558Z","iopub.execute_input":"2022-07-29T01:11:48.818923Z","iopub.status.idle":"2022-07-29T01:11:50.111112Z","shell.execute_reply.started":"2022-07-29T01:11:48.818871Z","shell.execute_reply":"2022-07-29T01:11:50.109732Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Data Scaling\n\nA Gaussian Mixture model assumes that each variable follows a gaussian distribution. Examining the dataset below we can see this isnt the case with many variables being skewed. In order to correct this I used the  sklearn power transformer implementing the yeo-johnson method, this method doesnt require all datapoints to be positive (like a Box-Cox transform) and resulted in a better score than other transformerms.","metadata":{}},{"cell_type":"code","source":"#Shaping to appropriate format\ndf_copy = df.drop(columns = ['id'])\n\nscaler = preprocessing.PowerTransformer(method = 'yeo-johnson', standardize=True).fit(df_copy.values)\nscaled_df = pd.DataFrame(scaler.transform(df_copy.values), index = df_copy.index, columns = df_copy.columns)\n# p_vals = []\n# for col in df_copy.columns:\n#     pre_transform = stats.shapiro(df[col]).pvalue\n#     post_transform = stats.shapiro(scaled_df[col]).pvalue\n#     p_vals.append([col, pre_transform, post_transform])\n\n# p_val_df = pd.DataFrame(p_vals, columns = ['Variable', 'Pre-Transform', 'Post-Transform'])\n# print(p_val_df.sort_values(by=['Pre-Transform']))\n\n\n# melted_df_pre = df_copy.melt(value_vars = df_copy.columns,\n#                     value_name = 'Value', var_name = 'Variable')\n# melted_df_post = scaled_df.melt(value_vars = df_copy.columns,\n#                     value_name = 'Value', var_name = 'Variable')\n# melted_df_pre['Transform'] = 'No Transform'\n# melted_df_post['Transform'] = 'yeo-johnson'\n# melted_df = pd.concat([melted_df_pre, melted_df_post], ignore_index = True)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-29T01:11:50.113799Z","iopub.execute_input":"2022-07-29T01:11:50.114298Z","iopub.status.idle":"2022-07-29T01:11:54.148889Z","shell.execute_reply.started":"2022-07-29T01:11:50.114249Z","shell.execute_reply":"2022-07-29T01:11:54.147544Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# #melted_df.head(n = 10)\n# sns.set(rc = {'figure.figsize':(15,12)})\n# v = sns.FacetGrid(melted_df, col='Variable', hue = 'Transform', height=2.5, col_wrap=5, sharex = False)\n# v.map(sns.histplot, 'Value', alpha = 0.5).add_legend()\n# v.tight_layout","metadata":{"execution":{"iopub.status.busy":"2022-07-29T01:11:54.151227Z","iopub.execute_input":"2022-07-29T01:11:54.151584Z","iopub.status.idle":"2022-07-29T01:11:54.157204Z","shell.execute_reply.started":"2022-07-29T01:11:54.151553Z","shell.execute_reply":"2022-07-29T01:11:54.156019Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Filtering for Best Variables\n\nBasing the approach off the notebook here: https://www.kaggle.com/code/ricopue/tps-jul22-clusters-and-lgb\n\nwe're going to take a subset of our factors to use for model training.\n\n\n","metadata":{}},{"cell_type":"code","source":"# sns.set(rc = {'figure.figsize':(15,12)})\n# sns.set_style('white')\n# heatmap = sns.heatmap(scaled_df.corr(), annot=False, cmap='BrBG',)\n# heatmap.set_title('Variable Correlation', fontdict={'fontsize':26}, pad=16);\nscaled_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-07-29T01:11:54.158398Z","iopub.execute_input":"2022-07-29T01:11:54.158763Z","iopub.status.idle":"2022-07-29T01:11:54.198806Z","shell.execute_reply.started":"2022-07-29T01:11:54.158732Z","shell.execute_reply":"2022-07-29T01:11:54.197546Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"       f_00      f_01      f_02      f_03      f_04      f_05      f_06  \\\n0 -0.389230 -0.917652  0.647948  0.590717 -0.824836  0.734685  2.259470   \n1 -0.688368 -0.458647  0.653182  0.995359 -1.644030  0.864898 -0.085604   \n2  0.805709  0.319397 -1.166935 -0.622421  0.108371  0.785018  1.990489   \n3 -0.500469  0.223997  0.262677  0.234061  0.417047 -1.218768  0.144455   \n4 -0.670427 -1.044482 -0.270854 -1.833338 -0.285955 -1.849243  0.787627   \n\n       f_07      f_08      f_09  ...      f_19      f_20      f_21      f_22  \\\n0 -0.977987  1.383372  1.039938  ... -0.472922 -0.753925 -0.763110 -0.707876   \n1 -0.977987 -0.875405 -0.179925  ... -0.423594 -0.088164 -1.777545 -0.535582   \n2  0.021718  1.017648 -0.394246  ... -0.408425 -1.598612  1.194423  2.203065   \n3  0.286548 -1.213526  0.917564  ...  0.620278  1.283827  0.532884  0.731623   \n4  0.756900  0.187543 -0.394246  ... -1.614933 -0.432406  0.321899  0.228337   \n\n       f_23      f_24      f_25      f_26      f_27      f_28  \n0  0.911477 -0.678852  0.768543  0.960344  1.042536  0.694234  \n1  0.453824  1.031505 -0.117686 -0.550783  0.367242 -1.636652  \n2  0.086974 -1.519163 -0.568662  0.978900 -0.926277 -2.296373  \n3 -1.218086  0.826492 -1.173592 -0.395085 -0.100021  0.326682  \n4 -1.482684  0.847999 -0.613935  1.164389 -0.374203 -1.160058  \n\n[5 rows x 29 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f_00</th>\n      <th>f_01</th>\n      <th>f_02</th>\n      <th>f_03</th>\n      <th>f_04</th>\n      <th>f_05</th>\n      <th>f_06</th>\n      <th>f_07</th>\n      <th>f_08</th>\n      <th>f_09</th>\n      <th>...</th>\n      <th>f_19</th>\n      <th>f_20</th>\n      <th>f_21</th>\n      <th>f_22</th>\n      <th>f_23</th>\n      <th>f_24</th>\n      <th>f_25</th>\n      <th>f_26</th>\n      <th>f_27</th>\n      <th>f_28</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.389230</td>\n      <td>-0.917652</td>\n      <td>0.647948</td>\n      <td>0.590717</td>\n      <td>-0.824836</td>\n      <td>0.734685</td>\n      <td>2.259470</td>\n      <td>-0.977987</td>\n      <td>1.383372</td>\n      <td>1.039938</td>\n      <td>...</td>\n      <td>-0.472922</td>\n      <td>-0.753925</td>\n      <td>-0.763110</td>\n      <td>-0.707876</td>\n      <td>0.911477</td>\n      <td>-0.678852</td>\n      <td>0.768543</td>\n      <td>0.960344</td>\n      <td>1.042536</td>\n      <td>0.694234</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.688368</td>\n      <td>-0.458647</td>\n      <td>0.653182</td>\n      <td>0.995359</td>\n      <td>-1.644030</td>\n      <td>0.864898</td>\n      <td>-0.085604</td>\n      <td>-0.977987</td>\n      <td>-0.875405</td>\n      <td>-0.179925</td>\n      <td>...</td>\n      <td>-0.423594</td>\n      <td>-0.088164</td>\n      <td>-1.777545</td>\n      <td>-0.535582</td>\n      <td>0.453824</td>\n      <td>1.031505</td>\n      <td>-0.117686</td>\n      <td>-0.550783</td>\n      <td>0.367242</td>\n      <td>-1.636652</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.805709</td>\n      <td>0.319397</td>\n      <td>-1.166935</td>\n      <td>-0.622421</td>\n      <td>0.108371</td>\n      <td>0.785018</td>\n      <td>1.990489</td>\n      <td>0.021718</td>\n      <td>1.017648</td>\n      <td>-0.394246</td>\n      <td>...</td>\n      <td>-0.408425</td>\n      <td>-1.598612</td>\n      <td>1.194423</td>\n      <td>2.203065</td>\n      <td>0.086974</td>\n      <td>-1.519163</td>\n      <td>-0.568662</td>\n      <td>0.978900</td>\n      <td>-0.926277</td>\n      <td>-2.296373</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.500469</td>\n      <td>0.223997</td>\n      <td>0.262677</td>\n      <td>0.234061</td>\n      <td>0.417047</td>\n      <td>-1.218768</td>\n      <td>0.144455</td>\n      <td>0.286548</td>\n      <td>-1.213526</td>\n      <td>0.917564</td>\n      <td>...</td>\n      <td>0.620278</td>\n      <td>1.283827</td>\n      <td>0.532884</td>\n      <td>0.731623</td>\n      <td>-1.218086</td>\n      <td>0.826492</td>\n      <td>-1.173592</td>\n      <td>-0.395085</td>\n      <td>-0.100021</td>\n      <td>0.326682</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.670427</td>\n      <td>-1.044482</td>\n      <td>-0.270854</td>\n      <td>-1.833338</td>\n      <td>-0.285955</td>\n      <td>-1.849243</td>\n      <td>0.787627</td>\n      <td>0.756900</td>\n      <td>0.187543</td>\n      <td>-0.394246</td>\n      <td>...</td>\n      <td>-1.614933</td>\n      <td>-0.432406</td>\n      <td>0.321899</td>\n      <td>0.228337</td>\n      <td>-1.482684</td>\n      <td>0.847999</td>\n      <td>-0.613935</td>\n      <td>1.164389</td>\n      <td>-0.374203</td>\n      <td>-1.160058</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 29 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"best_data =['f_00', 'f_01', 'f_02', 'f_03', 'f_04', 'f_05', 'f_06', 'f_22','f_23', 'f_24', 'f_25','f_26','f_27', 'f_28']\nscaled_df = scaled_df[best_data]","metadata":{"execution":{"iopub.status.busy":"2022-07-29T01:11:54.200916Z","iopub.execute_input":"2022-07-29T01:11:54.201375Z","iopub.status.idle":"2022-07-29T01:11:54.211019Z","shell.execute_reply.started":"2022-07-29T01:11:54.201329Z","shell.execute_reply":"2022-07-29T01:11:54.209963Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Parameter Tuning\n\nThe first parameter we need to identify is the number of clusters to predict. We'll do this by taking a subset of the data(to reduce training time) and training a model for between 2-15 clusters. \n\nTo compare model performance we'll use the Silhouette score explained here: https://en.wikipedia.org/wiki/Silhouette_(clustering). \n\nGiven that increasing the number of groups will naturally lead to a lower silhouette score we'll use the elbow method explained here: https://en.wikipedia.org/wiki/Elbow_method_(clustering) to look at when the rate of change reduces as we increase the number of clusters.\n\nThe best leaderboard score was a result of using n = 7 clusters","metadata":{}},{"cell_type":"code","source":"# sample_df = scaled_df.sample(n = 5000)\n# clusters = range(2,15)\n# scores = []\n\n# for i in clusters:\n#     gm = GaussianMixture(n_components=i, n_init=5, init_params='kmeans',\n#                         verbose = 0)\n#     gm_prediction = gm.fit_predict(sample_df)\n#     # Calculate Silhoutte Score and append to a list\n#     score = metrics.silhouette_score(sample_df, gm_prediction, metric='euclidean')\n#     scores.append(score)\n#     print('Number of Clusters: ', i, ' Score: ', score)\n  \n\n# plt.plot(clusters, scores, 'bo-')\n# plt.xlabel('Number of Clusters')\n# plt.ylabel('Silhouette Score')\n# plt.title('Silhouette Score by Cluster Count')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-29T01:11:54.212019Z","iopub.execute_input":"2022-07-29T01:11:54.212364Z","iopub.status.idle":"2022-07-29T01:11:54.221897Z","shell.execute_reply.started":"2022-07-29T01:11:54.212332Z","shell.execute_reply":"2022-07-29T01:11:54.221025Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Training Full Models\nUsing the identified number of clustes we'll train 3 models, a BayessianGaussianMixture, GaussianMixture, and MiniBatchKmeans.","metadata":{}},{"cell_type":"code","source":"!pip install scikit-learn-extra\n","metadata":{"execution":{"iopub.status.busy":"2022-07-29T01:11:54.223263Z","iopub.execute_input":"2022-07-29T01:11:54.223611Z","iopub.status.idle":"2022-07-29T01:12:09.991051Z","shell.execute_reply.started":"2022-07-29T01:11:54.223579Z","shell.execute_reply":"2022-07-29T01:12:09.989685Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Collecting scikit-learn-extra\n  Downloading scikit_learn_extra-0.2.0-cp37-cp37m-manylinux2010_x86_64.whl (1.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[?25hRequirement already satisfied: scikit-learn>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn-extra) (1.0.2)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn-extra) (1.7.3)\nRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn-extra) (1.21.6)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.23.0->scikit-learn-extra) (1.1.0)\nInstalling collected packages: scikit-learn-extra\nSuccessfully installed scikit-learn-extra-0.2.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn_extra.cluster import KMedoids\n","metadata":{"execution":{"iopub.status.busy":"2022-07-29T01:12:09.993106Z","iopub.execute_input":"2022-07-29T01:12:09.994573Z","iopub.status.idle":"2022-07-29T01:12:10.011202Z","shell.execute_reply.started":"2022-07-29T01:12:09.994516Z","shell.execute_reply":"2022-07-29T01:12:10.009863Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\n\n# model_1 = BayesianGaussianMixture(n_components=6, n_init=5, verbose = 0.5,tol = 0.0001, max_iter = 200).fit(scaled_df)\n# model_2 = GaussianMixture(n_components=6, n_init=5, verbose = 0.5,tol = 0.0001, max_iter = 200).fit(scaled_df)\n# model_3 = MiniBatchKMeans(n_clusters=6).fit(scaled_df)\nmodel_4 = KMedoids(n_clusters=6, max_iter=2, method='pam', random_state=42).fit(scaled_df)\n\n# score_1 = metrics.silhouette_score(scaled_df, model_1.predict(scaled_df), metric='euclidean')\nscore_4 = metrics.silhouette_score(scaled_df, model_4.predict(scaled_df), metric='euclidean')\n\n# print(score_1)\nprint(score_4)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T01:12:10.015104Z","iopub.execute_input":"2022-07-29T01:12:10.015511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensembling Results","metadata":{}},{"cell_type":"code","source":"#Storing models\npredictions = pd.DataFrame(np.array([model_1.predict(scaled_df),\n                           model_4.predict(scaled_df)]).T, columns = ['BGM', 'KMedoids'])\npredictions.head(n=10)\nprint(int(round(score_3*100, 0)))\ntest = predictions['KMedoids']\nprint(type(test))\nprint(len(predictions['KMedoids'] ))\n\n\nagg_prediction = []\nfor index, row in predictions.iterrows():\n    #print(row['c1'], row['c2'])\n    agg_prediction.append(mode([row['BGM']]* int(round((score_1**-1)*100,0)) + \n                               [row['KMedoids']] * int(round((score_3**-1)*100, 0))))\npredictions['Weighted_Pred'] = agg_prediction\npredictions.head(n=10)\n    \n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing Results\n\nUsing principle component analysis we can reduce our dataset to 2 dimensions and visualize the clustering below. Unfortunately we cant capture all the variance of the dataset in two dimensions as shown by plotting the explained variance of each principle component so our 2d clustering visualization isnt perfect.","metadata":{}},{"cell_type":"code","source":"prediction = predictions['Weighted_Pred']\nscore = metrics.silhouette_score(scaled_df, prediction, metric='euclidean')\npca = PCA()\npca.fit_transform(scaled_df)\npca2 = PCA(n_components=2)\npca2.fit(scaled_df)\n\n#Visualizing variance explanation of each principle component\nvariance = pca.explained_variance_\nplt.figure(figsize=(8, 6))\nplt.bar(range(len(variance)), variance, alpha=0.5, align='center', label='individual variance')\nplt.legend()\nplt.ylabel('Variance ratio')\nplt.xlabel('Principal components')\nplt.show()\n\n#Visualizing clustering\nsns.set(rc = {'figure.figsize':(8,6)})\nscaled_pca = pd.DataFrame(pca2.transform(scaled_df), columns = ['PCA1', 'PCA2'])\nscaled_pca['Group Prediction'] = prediction\nsns.scatterplot(data = scaled_pca, x = 'PCA1', y = 'PCA2', hue = 'Group Prediction').set(title = 'Clustering Group Visualization with PCA')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Making Prediction and Writing to File","metadata":{}},{"cell_type":"code","source":"labels = df['id']\nsubmission = pd.DataFrame(np.array([labels, prediction]).T,\n                                 columns = ['Id', 'Predicted'])\nsubmission.to_csv('ensemble.csv', index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}